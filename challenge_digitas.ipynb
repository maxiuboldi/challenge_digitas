{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/maxiuboldi/test_ml/blob/main/test_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Digitas - Examen DS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \\\n",
    "    category-encoders==2.6.2 \\\n",
    "    cloudpickle==3.0.0 \\\n",
    "    colorama==0.4.6 \\\n",
    "    contourpy==1.1.1 \\\n",
    "    cycler==0.12.1 \\\n",
    "    feature-engine==1.6.2 \\\n",
    "    fonttools==4.43.1 \\\n",
    "    jinja2==3.1.2 \\\n",
    "    joblib==1.3.2 \\\n",
    "    kiwisolver==1.4.5 \\\n",
    "    lightgbm==4.1.0 \\\n",
    "    llvmlite==0.41.1 \\\n",
    "    markupsafe==2.1.3 \\\n",
    "    matplotlib==3.8.0 \\\n",
    "    numba==0.58.1 \\\n",
    "    numpy==1.26.1 \\\n",
    "    packaging==23.2 \\\n",
    "    pandas==2.1.1 \\\n",
    "    patsy==0.5.3 \\\n",
    "    pillow==10.1.0 \\\n",
    "    pyparsing==3.1.1 \\\n",
    "    python-dateutil==2.8.2 \\\n",
    "    pytz==2023.3.post1 \\\n",
    "    scikit-learn==1.3.2 \\\n",
    "    scikit-plot==0.3.7 \\\n",
    "    scipy==1.11.3 \\\n",
    "    seaborn==0.13.0 \\\n",
    "    setuptools-scm==8.0.4 \\\n",
    "    setuptools==68.2.2 \\\n",
    "    shap==0.43.0 \\\n",
    "    six==1.16.0 \\\n",
    "    slicer==0.0.7 \\\n",
    "    statsmodels==0.14.0 \\\n",
    "    threadpoolctl==3.2.0 \\\n",
    "    tomli==2.0.1 \\\n",
    "    tqdm==4.66.1 \\\n",
    "    typing-extensions==4.8.0 \\\n",
    "    tzdata==2023.3 \\\n",
    "    xgboost==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from scikitplot.helpers import binary_ks_curve\n",
    "from scikitplot.metrics import plot_ks_statistic\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from category_encoders import CatBoostEncoder\n",
    "from feature_engine.imputation import EndTailImputer\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('FlightDelays_Data_3.0[67].csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan algunos valores ausentes en ciertas variables, pero son mínimos. Se asume que es posible que, eventualmente, en una implementación también pudieran surgir por lo que se mantienen y se procederá a su imputación.\n",
    "\n",
    "No obstante, se observa un registro en la variable objetivo que también presenta un valor ausente. Siendo el target, se excluye ese registro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[~dataset['Canceled'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa un problema desbalanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(\n",
    "    data=dataset,\n",
    "    x='Canceled'\n",
    ")\n",
    "ax.set_xlabel('Target')\n",
    "ax.set_ylabel('Cantidad')\n",
    "ax.set_title(\n",
    "    f'Distribución de Vuelos Cancelados'\n",
    ")\n",
    "ax.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    percentage = (height / len(dataset)) * 100\n",
    "    ax.annotate(\n",
    "        text=f'{percentage:.1f}%',\n",
    "        xy=(p.get_x() + p.get_width() / 2., height),\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        xytext=(0, 5),\n",
    "        textcoords='offset points'\n",
    "    )\n",
    "\n",
    "ax.set_xticklabels(['No cancelado', 'Cancelado'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En principio parece ser que la aerolínea AA es la que mayor proporción de cancelados manifiesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.countplot(\n",
    "    data=dataset,\n",
    "    x='UniqueCarrier',\n",
    "    hue='Canceled'\n",
    ")\n",
    "\n",
    "plt.title('Distribución de Vuelos Cancelados por Aerolínea')\n",
    "plt.xlabel('Aerolínea')\n",
    "plt.ylabel('Cantidad de vuelos')\n",
    "\n",
    "legend_labels, _= ax.get_legend_handles_labels()\n",
    "ax.legend(legend_labels, ['No cancelado', 'Cancelado'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_flights = dataset['UniqueCarrier'].value_counts()\n",
    "canceled_flights = dataset[dataset['Canceled']\n",
    "                           == 1]['UniqueCarrier'].value_counts()\n",
    "\n",
    "proportion_canceled = (canceled_flights / total_flights).reset_index()\n",
    "proportion_canceled.columns = ['UniqueCarrier', 'ProportionCanceled']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=proportion_canceled,\n",
    "    x='UniqueCarrier',\n",
    "    y='ProportionCanceled'\n",
    ")\n",
    "plt.title('Proporción de Vuelos Cancelados por Aerolínea')\n",
    "plt.xlabel('Aerolínea')\n",
    "plt.ylabel('Proporción de vuelos cancelados')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = list(dataset.select_dtypes(exclude=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[[x for x in num_cols if x not in ['Canceled']]].hist(\n",
    "    figsize=(10, 10),\n",
    "    grid=False\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "for index, column in enumerate([x for x in num_cols if x not in ['Canceled']]):\n",
    "    plt.subplot(math.ceil(len(num_cols)/2), 2, index+1)\n",
    "    sns.boxplot(y=column, x='Canceled', hue='Canceled', data=dataset,\n",
    "                palette='dark', showfliers=False, legend=False)\n",
    "    plt.title('Distribución de \"{}\" según \"Canceled\"'.format(column))\n",
    "    plt.xticks(ticks=[0, 1], labels=['No cancelado', 'Cancelado'])\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan ciertos valores atípicos en las distribuciones que sugerirían profundizar en las lógicas de construcción del conjunto de datos. Por ejemplo, por qué podría producirse que el tiempo de viaje planificado (SchedElapsedTime) o la distancia (Distance) presenten valores negativos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombramos para tener identificada la variable target\n",
    "dataset = dataset.rename({'Canceled': 'Target'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Funciones útiles'''\n",
    "\n",
    "def compute_metrics(\n",
    "        y_true: List[float],\n",
    "        y_score: List[float]\n",
    "    ) -> Dict[str, float]:\n",
    "    '''\n",
    "    Computes various evaluation metrics based on true labels and scores.\n",
    "    '''\n",
    "    return {\n",
    "        'roc_score': roc_auc_score(y_true, y_score),\n",
    "        'ks': binary_ks_curve(y_true, y_score)[3],\n",
    "        'pr_score': average_precision_score(y_true, y_score)\n",
    "    }\n",
    "\n",
    "def compute_roc_optimal_cutoff(\n",
    "        y_true: List[float],\n",
    "        y_score: List[float]\n",
    "    ) -> float:\n",
    "    '''\n",
    "    Compute the optimal cutoff threshold based on the Receiver Operating\n",
    "    Characteristic (ROC) curve (Youden's index).\n",
    "    '''\n",
    "\n",
    "    fpr, tpr, thresh = roc_curve(y_true, y_score)\n",
    "    idx = np.arange(len(tpr))\n",
    "    roc = pd.DataFrame(\n",
    "        {\n",
    "            'tf': pd.Series(tpr - (1 - fpr), index=idx),\n",
    "            'threshold': pd.Series(thresh, index=idx)\n",
    "        }\n",
    "    )\n",
    "    roc_t = roc.iloc[(roc.tf - 0).abs().argsort()[:1]]\n",
    "    return float(roc_t['threshold'].values[0])\n",
    "\n",
    "def compute_ks_optimal_cutoff(\n",
    "        y_true: List[float],\n",
    "        y_score: List[float]\n",
    "    ) -> float:\n",
    "    '''\n",
    "    Compute the optimal cutoff threshold based on the Kolmogorov-Smirnov\n",
    "    (KS) statistic.\n",
    "    '''\n",
    "    return binary_ks_curve(y_true, y_score)[4]\n",
    "\n",
    "def compute_optimal_cutoff(\n",
    "        y_true: List[float],\n",
    "        y_score: List[float],\n",
    "        method: str = 'ks'\n",
    "    ) -> float:\n",
    "    '''\n",
    "    Compute the optimal cutoff threshold based on the specified method\n",
    "    and test date.\n",
    "    '''\n",
    "    if method == 'roc':\n",
    "        return compute_roc_optimal_cutoff(y_true, y_score)\n",
    "    elif method == 'ks':\n",
    "        return compute_ks_optimal_cutoff(y_true, y_score)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported method: {method}')\n",
    "\n",
    "def plot_ks(\n",
    "        y_true: List[float],\n",
    "        y_score: List[float],\n",
    "        figsize: tuple = (10, 6),\n",
    "        test_date: str = 'N/D'\n",
    "    ):\n",
    "    '''\n",
    "    Plot the Kolmogorov-Smirnov (KS) statistic.\n",
    "    '''\n",
    "    pred_scores = np.column_stack(\n",
    "        (1 - y_score, y_score))\n",
    "\n",
    "    plot_ks_statistic(\n",
    "        y_true,\n",
    "        pred_scores,\n",
    "        title=f'Estadístico KS - {test_date}',\n",
    "        figsize=figsize\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(\n",
    "        model,\n",
    "        top_n: int = 10,\n",
    "        figsize: tuple = (10, 6)\n",
    "    ):\n",
    "    '''\n",
    "    Plot the top feature importances.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        if hasattr(model, 'feature_names_in_'):\n",
    "            feature_importances = pd.Series(\n",
    "                model.feature_importances_ /\n",
    "                model.feature_importances_.sum(),\n",
    "                index=model.feature_names_in_\n",
    "            )\n",
    "        elif hasattr(model, 'feature_name_'):\n",
    "            feature_importances = pd.Series(\n",
    "                model.feature_importances_ /\n",
    "                model.feature_importances_.sum(),\n",
    "                index=model.feature_name_\n",
    "            )\n",
    "        elif hasattr(model, 'feature_names_'):\n",
    "            feature_importances = pd.Series(\n",
    "                model.feature_importances_ /\n",
    "                model.feature_importances_.sum(),\n",
    "                index=model.feature_names_\n",
    "            )\n",
    "        else:\n",
    "            feature_importances = pd.Series(\n",
    "                model.feature_importances_ /\n",
    "                model.feature_importances_.sum(),\n",
    "                index=[str(i) for i in range(\n",
    "                    len(model.feature_importances_))]\n",
    "            )\n",
    "    except AttributeError:\n",
    "        print(\n",
    "            (\n",
    "                'Cannot calculate feature importance. '\n",
    "                'Is your model a decision tree object?'\n",
    "            )\n",
    "        )\n",
    "    feature_importances = feature_importances.sort_values(ascending=False)\n",
    "    top_features = feature_importances[:top_n]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(x=top_features.values, y=top_features.index)\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.ylabel('Variable')\n",
    "    plt.title(f'Top {top_n} - Importancia de Variables')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "        y_true: List[float],\n",
    "        y_score: List[float],\n",
    "        test_date='N/D',\n",
    "        threshold=0.5,\n",
    "        display_labels=None,\n",
    "        figsize=(10, 6),\n",
    "        normalize=None\n",
    "    ):\n",
    "    '''\n",
    "    Plot the confusion matrix.\n",
    "    '''\n",
    "\n",
    "    test_target = y_true\n",
    "    pred_target = np.where(y_score > threshold, 1, 0)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        test_target,\n",
    "        pred_target,\n",
    "        display_labels=display_labels,\n",
    "        cmap='Blues',\n",
    "        values_format='.0f' if normalize is None else None,\n",
    "        normalize=normalize,\n",
    "        colorbar=False\n",
    "    )\n",
    "    disp.ax_.set_title(f\"Matriz de Confusión - {test_date}\")\n",
    "    disp.figure_.set_size_inches(figsize)\n",
    "    plt.show()\n",
    "\n",
    "def plot_calibration_curve(\n",
    "        y_true: List[float],\n",
    "        y_score: List[float],\n",
    "        test_date='N/D',\n",
    "        figsize=(10, 6)\n",
    "    ):\n",
    "    '''\n",
    "    Plot the calibration curve.\n",
    "    '''\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    disp = CalibrationDisplay.from_predictions(\n",
    "        y_true,\n",
    "        y_score\n",
    "    )\n",
    "    disp.ax_.set_title(f\"Curva de Calibración - {test_date}\")\n",
    "\n",
    "    handles, labels = disp.ax_.get_legend_handles_labels()\n",
    "    disp.ax_.legend(handles, labels, loc='best')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_shap_importance(\n",
    "        model,\n",
    "        train_data,\n",
    "        top_n: int = 10,\n",
    "        figsize: tuple = (10, 6)\n",
    "    ):\n",
    "    '''\n",
    "    Plot the top feature importances.\n",
    "    '''\n",
    "\n",
    "    explainer = shap.TreeExplainer(\n",
    "        model=model,\n",
    "        feature_perturbation='tree_path_dependent',\n",
    "        model_output='raw'\n",
    "    )\n",
    "    shap_values = explainer.shap_values(train_data)\n",
    "    shap.summary_plot(\n",
    "        # shap_values[1],\n",
    "        shap_values,\n",
    "        train_data,\n",
    "        plot_type='violin',\n",
    "        max_display=top_n,\n",
    "        plot_size=figsize,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'Importancia SHAP - Top {top_n}', fontsize=16, y=1.05)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 8888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_model = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(\n",
    "    dataset_model.drop('Target', axis=1),\n",
    "    dataset_model['Target'],\n",
    "    test_size=0.1,\n",
    "    random_state=SEED,\n",
    "    stratify=dataset_model['Target']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = list(train_data.select_dtypes(['object']).columns)\n",
    "num_cols = list(train_data.select_dtypes(exclude=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "        ('Dummy', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('model', DummyClassifier()\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('LogisticRegression', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('scaler', SklearnTransformerWrapper(StandardScaler())\n",
    "                ),\n",
    "                ('model', LogisticRegression(\n",
    "                        penalty=None,\n",
    "                        solver='lbfgs',\n",
    "                        random_state=SEED\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('LassoLogisticRegression', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('scaler', SklearnTransformerWrapper(StandardScaler())\n",
    "                ),\n",
    "                ('model', LogisticRegression(\n",
    "                        penalty='l1',\n",
    "                        solver='saga',\n",
    "                        random_state=SEED\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('RidgeLogisticRegression', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('scaler', SklearnTransformerWrapper(StandardScaler())\n",
    "                ),\n",
    "                ('model', LogisticRegression(\n",
    "                        penalty='l2',\n",
    "                        solver='lbfgs',\n",
    "                        random_state=SEED\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('ElasticNetLogisticRegression', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('scaler', SklearnTransformerWrapper(StandardScaler())\n",
    "                ),\n",
    "                ('model', LogisticRegression(\n",
    "                        penalty='elasticnet',\n",
    "                        solver='saga',\n",
    "                        l1_ratio=0.5,\n",
    "                        random_state=SEED\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('KNeighbors', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('scaler', SklearnTransformerWrapper(MinMaxScaler())\n",
    "                ),\n",
    "                ('model', KNeighborsClassifier(n_jobs=-1),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('LightGBM', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('model', LGBMClassifier(\n",
    "                        random_state=SEED,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=-1\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('XGBoost', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('model', XGBClassifier(random_state=SEED, n_jobs=-1),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('ExtraTrees', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('model', ExtraTreesClassifier(\n",
    "                        random_state=SEED,\n",
    "                        n_jobs=-1\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('RandomForest', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('model', RandomForestClassifier(\n",
    "                        random_state=SEED,\n",
    "                        n_jobs=-1\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('SGDSVM', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('scaler', SklearnTransformerWrapper(StandardScaler())\n",
    "                ),\n",
    "                ('model', CalibratedClassifierCV(\n",
    "                        SGDClassifier(\n",
    "                            loss='hinge', # SVM\n",
    "                            n_jobs=-1,\n",
    "                            random_state=SEED\n",
    "                        ),\n",
    "                        method='sigmoid',\n",
    "                        cv=None,\n",
    "                        n_jobs=-1,\n",
    "                        ensemble=True\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', 'is_categorical_dtype')\n",
    "model_metrics_list = []\n",
    "\n",
    "for model, learner in tqdm(models, desc=r'Running Model Selection'):\n",
    "\n",
    "    metrics_list = []\n",
    "    for train_index, valid_index in StratifiedKFold(\n",
    "            n_splits=5, random_state=SEED, shuffle=True\n",
    "        ).split(\n",
    "        train_data, train_target):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_data_fold = train_data.iloc[train_index].copy()\n",
    "        train_target_fold = train_target.iloc[train_index].copy()\n",
    "\n",
    "        val_data_fold = train_data.iloc[valid_index].copy()\n",
    "        val_target_fold = train_target.iloc[valid_index].copy()\n",
    "\n",
    "        num_imputer = EndTailImputer(\n",
    "            imputation_method='iqr',\n",
    "            tail='left',\n",
    "            fold=5,\n",
    "            variables=num_cols\n",
    "        )\n",
    "        train_data_fold = num_imputer.fit_transform(train_data_fold)\n",
    "        val_data_fold = num_imputer.transform(val_data_fold)\n",
    "\n",
    "        cat_encoder = CatBoostEncoder(\n",
    "            cols=cat_cols,\n",
    "            drop_invariant=True\n",
    "        )\n",
    "        train_data_fold = cat_encoder.fit_transform(train_data_fold, \n",
    "                                                    train_target_fold)\n",
    "        val_data_fold = cat_encoder.transform(val_data_fold)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "            learner.fit(train_data_fold, train_target_fold)\n",
    "\n",
    "        pred_target_fold = learner.predict_proba(val_data_fold)[:, -1]\n",
    "\n",
    "        metrics_fold = compute_metrics(val_target_fold, pred_target_fold)\n",
    "        metrics_fold.update({'TT_secs': time.time() - start_time})\n",
    "        metrics_list.append(metrics_fold)\n",
    "\n",
    "    metrics = pd.DataFrame(metrics_list)\n",
    "    metrics.columns = metrics.columns.str.upper()\n",
    "    metrics = metrics.mean().to_frame().T\n",
    "    metrics.index = [model]\n",
    "\n",
    "    model_metrics_list.append(metrics)\n",
    "\n",
    "metrics = pd.concat(model_metrics_list).round(4)\n",
    "metrics.index.name = 'MODEL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.sort_values(\n",
    "    'PR_SCORE',\n",
    "    ascending=False\n",
    ").style.highlight_max(\n",
    "        subset=[col for col in metrics.columns if col != 'TT_SECS'],\n",
    "        color='green'\n",
    "    ).map(lambda x: 'background-color: gray', subset=['TT_SECS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('model', XGBClassifier(random_state=SEED, n_jobs=-1),\n",
    "                )\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(\n",
    "    train_data,\n",
    "    train_target\n",
    ")\n",
    "scores = pipeline.predict_proba(test_data)[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metrics(\n",
    "    y_true=test_target,\n",
    "    y_score=scores\n",
    ")\n",
    "for m, s in metrics.items():\n",
    "    print(f'{m}: {s:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importance(\n",
    "    model=pipeline['model'],\n",
    "    top_n=7,\n",
    "    figsize=(8, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_shap = pipeline['cat_encoder'].transform(train_data)\n",
    "train_data_shap = pipeline['num_imputer'].transform(train_data_shap)\n",
    "plot_shap_importance(\n",
    "    model=pipeline['model'],\n",
    "    train_data=train_data_shap,\n",
    "    top_n=7,\n",
    "    figsize=(8, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas variables para \"predecir\" no sirven porque a priori entiendo que no estarían disponibles al momento de una eventual inferencia.\n",
    "dataset_model = dataset.copy()\n",
    "dataset_model = dataset_model.drop(['ArrDelay', 'DepDelay'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(\n",
    "    dataset_model.drop('Target', axis=1),\n",
    "    dataset_model['Target'],\n",
    "    test_size=0.1,\n",
    "    random_state=SEED,\n",
    "    stratify=dataset_model['Target']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = list(train_data.select_dtypes(['object']).columns)\n",
    "num_cols = list(train_data.select_dtypes(exclude=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "        ('Dummy', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('model', DummyClassifier()\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('LogisticRegression', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('scaler', SklearnTransformerWrapper(StandardScaler())\n",
    "                ),\n",
    "                ('model', LogisticRegression(\n",
    "                        penalty=None,\n",
    "                        solver='lbfgs',\n",
    "                        random_state=SEED\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('LassoLogisticRegression', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('scaler', SklearnTransformerWrapper(StandardScaler())\n",
    "                ),\n",
    "                ('model', LogisticRegression(\n",
    "                        penalty='l1',\n",
    "                        solver='saga',\n",
    "                        random_state=SEED\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('RidgeLogisticRegression', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('scaler', SklearnTransformerWrapper(StandardScaler())\n",
    "                ),\n",
    "                ('model', LogisticRegression(\n",
    "                        penalty='l2',\n",
    "                        solver='lbfgs',\n",
    "                        random_state=SEED\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('ElasticNetLogisticRegression', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('scaler', SklearnTransformerWrapper(StandardScaler())\n",
    "                ),\n",
    "                ('model', LogisticRegression(\n",
    "                        penalty='elasticnet',\n",
    "                        solver='saga',\n",
    "                        l1_ratio=0.5,\n",
    "                        random_state=SEED\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('KNeighbors', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('scaler', SklearnTransformerWrapper(MinMaxScaler())\n",
    "                ),\n",
    "                ('model', KNeighborsClassifier(n_jobs=-1),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('LightGBM', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('model', LGBMClassifier(\n",
    "                        random_state=SEED,\n",
    "                        n_jobs=-1,\n",
    "                        verbose=-1\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('XGBoost', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('model', XGBClassifier(random_state=SEED, n_jobs=-1),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('ExtraTrees', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('model', ExtraTreesClassifier(\n",
    "                        random_state=SEED,\n",
    "                        n_jobs=-1\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('RandomForest', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('model', RandomForestClassifier(\n",
    "                        random_state=SEED,\n",
    "                        n_jobs=-1\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "        ('SGDSVM', Pipeline(\n",
    "            steps=[\n",
    "                ('cat_encoder', CatBoostEncoder(\n",
    "                        drop_invariant=True,\n",
    "                        random_state=SEED,\n",
    "                        cols=cat_cols,\n",
    "                    )\n",
    "                ),\n",
    "                ('num_imputer', EndTailImputer(\n",
    "                        imputation_method='iqr',\n",
    "                        tail='left',\n",
    "                        fold=5,\n",
    "                        variables=num_cols\n",
    "                    )\n",
    "                ),\n",
    "                ('scaler', SklearnTransformerWrapper(StandardScaler())\n",
    "                ),\n",
    "                ('model', CalibratedClassifierCV(\n",
    "                        SGDClassifier(\n",
    "                            loss='hinge', # SVM\n",
    "                            n_jobs=-1,\n",
    "                            random_state=SEED\n",
    "                        ),\n",
    "                        method='sigmoid',\n",
    "                        cv=None,\n",
    "                        n_jobs=-1,\n",
    "                        ensemble=True\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', 'is_categorical_dtype')\n",
    "model_metrics_list = []\n",
    "\n",
    "for model, learner in tqdm(models, desc=r'Running Model Selection'):\n",
    "\n",
    "    metrics_list = []\n",
    "    for train_index, valid_index in StratifiedKFold(\n",
    "            n_splits=5, random_state=SEED, shuffle=True\n",
    "        ).split(\n",
    "        train_data, train_target):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_data_fold = train_data.iloc[train_index].copy()\n",
    "        train_target_fold = train_target.iloc[train_index].copy()\n",
    "\n",
    "        val_data_fold = train_data.iloc[valid_index].copy()\n",
    "        val_target_fold = train_target.iloc[valid_index].copy()\n",
    "\n",
    "        num_imputer = EndTailImputer(\n",
    "            imputation_method='iqr',\n",
    "            tail='left',\n",
    "            fold=5,\n",
    "            variables=num_cols\n",
    "        )\n",
    "        train_data_fold = num_imputer.fit_transform(train_data_fold)\n",
    "        val_data_fold = num_imputer.transform(val_data_fold)\n",
    "\n",
    "        cat_encoder = CatBoostEncoder(\n",
    "            cols=cat_cols,\n",
    "            drop_invariant=True\n",
    "        )\n",
    "        train_data_fold = cat_encoder.fit_transform(train_data_fold, \n",
    "                                                    train_target_fold)\n",
    "        val_data_fold = cat_encoder.transform(val_data_fold)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "            learner.fit(train_data_fold, train_target_fold)\n",
    "\n",
    "        pred_target_fold = learner.predict_proba(val_data_fold)[:, -1]\n",
    "\n",
    "        metrics_fold = compute_metrics(val_target_fold, pred_target_fold)\n",
    "        metrics_fold.update({'TT_secs': time.time() - start_time})\n",
    "        metrics_list.append(metrics_fold)\n",
    "\n",
    "    metrics = pd.DataFrame(metrics_list)\n",
    "    metrics.columns = metrics.columns.str.upper()\n",
    "    metrics = metrics.mean().to_frame().T\n",
    "    metrics.index = [model]\n",
    "\n",
    "    model_metrics_list.append(metrics)\n",
    "\n",
    "metrics = pd.concat(model_metrics_list).round(4)\n",
    "metrics.index.name = 'MODEL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.sort_values(\n",
    "    'PR_SCORE',\n",
    "    ascending=False\n",
    ").style.highlight_max(\n",
    "        subset=[col for col in metrics.columns if col != 'TT_SECS'],\n",
    "        color='green'\n",
    "    ).map(lambda x: 'background-color: gray', subset=['TT_SECS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema en cuestión se define principalmente por 2 variables, ArrDelay (retraso en el arribo) y DepDelay (retraso en la partida). Estas 2 variables por si solas tienen una relación directa con la variable objetivo (cancelaciones). Según el análisis preliminar, valores bajos en estas variables parecen presentar una mayor correlación con el target. Excluyendo estas variables el problema en cuestión no parece ser explicado por las restantes y el modelado no se vuelve aconsejable.\n",
    "\n",
    "La principal pregunta acerca de un posible modelo productivo respecto de esta cuestión, sería, estas variables estarían disponibles al momento de una eventual predicción? o son variables recolectadas en forma posterior que no podrían utilizarse en la práctica posteriormente?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
